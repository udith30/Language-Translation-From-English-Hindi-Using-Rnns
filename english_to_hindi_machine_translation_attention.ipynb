{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BDx68ZADntY"
      },
      "source": [
        "##                                                Deep Learning PROJECT "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "TP8yTcnjDnta"
      },
      "source": [
        "## The goal of this notebook is to introduce sequence to sequence language translation (seq2seq).\n",
        "The notebook deals with a sequence to sequence model for English to Hindi translation. After training the model one will be able to input a English sentence and get back its Hindi translation.\n",
        "\n",
        ">RNNs are also capable of doing natural language translation, aka. machine translation. It involves two RNNs, one for the source language and one for the target language. One of them is called an encoder, and the other one decoder. The reason is that, the first one encodes the sentence into a vector and the second one converts the encoded vector into a sentence in target language. The decoder is a separete RNN. Given the encoded sentence, it produces the translated sentence in target language. Attention lets the decoder to focus on specific parts of the input sentence for each output word. This helps the input and output sentences to align with one another.\n",
        "\n",
        "We obtained the dataset used from Kaggle: https://www.kaggle.com/aiswaryaramachandran/hindienglish-corpora\n",
        "\n",
        "<h2> References: </h2>\n",
        "<li></a> Sequence to Sequence Learning with Neural Networks (Research Publication)</li>\n",
        "<li></a> https://www.tensorflow.org/tutorials/text/nmt_with_attention </li>\n",
        "<li></a> Using stochastic computation graphs formalism for optimization of sequence-to-sequence model (Research Publication) </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfjR-jRLDnta"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w0Ix6NDoDnta"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import io\n",
        "import time\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "        \n",
        "PATH = \"/content/Hindi_English_Truncated_Corpus.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mzM-zmwDntc"
      },
      "source": [
        "## Preprocess English and Hindi sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_eW04ZteDntc"
      },
      "outputs": [],
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    return w\n",
        "\n",
        "def hindi_preprocess_sentence(w):\n",
        "    w = w.rstrip().strip()\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rXv9nNh4Dntc"
      },
      "outputs": [],
      "source": [
        "def create_dataset(path=PATH):\n",
        "    lines=pd.read_csv(path,encoding='utf-8')\n",
        "    lines=lines.dropna()\n",
        "    lines = lines[lines['source']=='ted']\n",
        "    en = []\n",
        "    hd = []\n",
        "    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\n",
        "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
        "        en_1.append('<end>')\n",
        "        en_1.insert(0, '<start>')\n",
        "        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n",
        "        hd_1.append('<end>')\n",
        "        hd_1.insert(0, '<start>')\n",
        "        en.append(en_1)\n",
        "        hd.append(hd_1)\n",
        "    return hd, en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZYvB_A83Dntc"
      },
      "outputs": [],
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYG3usjUDntc"
      },
      "source": [
        "### Tokenization of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VpeY6bQADntc"
      },
      "outputs": [],
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "  return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hvW1L1kADntd"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path=PATH):\n",
        "    targ_lang, inp_lang = create_dataset(path)\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3q_Srno6Dntd"
      },
      "outputs": [],
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN-aB_mpDntd"
      },
      "source": [
        "### Create Train and Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tneb_bhDntd",
        "outputId": "5a7cb114-f186-4347-99e9-2904967b2cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31904 31904 7977 7977\n"
          ]
        }
      ],
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rXMuoZ_Dntd",
        "outputId": "4d744954-9160-4898-84ab-dc2e6ea9ba5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "126 ----> why\n",
            "7 ----> a\n",
            "120 ----> lot\n",
            "6 ----> of\n",
            "40 ----> people\n",
            "19 ----> have\n",
            "2547 ----> trouble\n",
            "573 ----> seeing\n",
            "895 ----> themselves\n",
            "32 ----> as\n",
            "2103 ----> leaders .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "199 ----> क्यों\n",
            "31 ----> बहुत\n",
            "181 ----> सारे\n",
            "66 ----> लोग\n",
            "210 ----> खुद\n",
            "11 ----> को\n",
            "1085 ----> नेतृत्व\n",
            "4 ----> के\n",
            "2354 ----> काबिल\n",
            "15 ----> नहीं\n",
            "764 ----> समझते\n",
            "52 ----> हैं।\n",
            "2 ----> <end>\n"
          ]
        }
      ],
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "    \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RCjw42vDntd"
      },
      "source": [
        "### Create Dataset\n",
        "> We are using minimal configuration as the notebbok is not focussed on metrics performance but rather the implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xe1Pw96IDntd"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 128\n",
        "units = 256\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkLxY6hADnte"
      },
      "source": [
        "## Encoder Decoder with Attention Model\n",
        "\n",
        "> Encoder Decoder with Attention model is a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. It uses a multilayered Gated Recurrent Unit (GRU) to map the input sequence to a vector of a fixed dimensionality, and then another deep GRU to decode the target sequence from the vector.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Vlad_Zhukov2/publication/321210603/figure/fig1/AS:642862530191361@1530281779831/An-example-of-sequence-to-sequence-model-with-attention-Calculation-of-cross-entropy.png\" width=\"800\" alt=\"attention mechanism\">\n",
        "\n",
        "\n",
        "> A sequence to sequence model has two parts – an encoder and a decoder. Both the parts are practically two different neural network models combined into one giant network. the task of an encoder network is to understand the input sequence, and create a smaller dimensional representation of it. This representation is then forwarded to a decoder network which generates a sequence of its own that represents the output. The input is put through an encoder model which gives us the encoder output. Here, each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. We use Bahdanau attention for the encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0ueLT9HDnte"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Arlwvt64Dnte"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir-roFEyDnte"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LVTpj96kDnte"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se3CLhApDntf"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E2tNkVldDntf"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "    return x, state, attention_weights\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJaGq4IDntf"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nzgU6tdRDntf"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "#   print(type(mask))\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "seO2Evt4Dntf"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_WZSumVDntf"
      },
      "source": [
        "## Training\n",
        "\n",
        ">1. Pass *input* through *encoder* to get *encoder output*..\n",
        ">2. Then encoder output, encoder hidden state and the decoder input is passed to decoder.\n",
        ">3. Decoder returns *predictions* and *decoder hidden state*.\n",
        ">4. Decoder hidden state is then passed back to model.\n",
        ">5. Predictions are used to calculate loss.\n",
        ">6. Use *teacher forcing* (technique where the target word is passed as the next input to the decoder) for the next input to the decoder.\n",
        ">7. Calculate gradients and apply it to *optimizer* for backpropogation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "63_3DNDyDntf"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    # Teacher forcing\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))      \n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkievgyhDntf",
        "outputId": "dec25cde-b3dd-4f5d-ba06-f69875141c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.0074\n",
            "Epoch 1 Batch 100 Loss 1.8105\n",
            "Epoch 1 Batch 200 Loss 1.8504\n",
            "Epoch 1 Batch 300 Loss 2.0275\n",
            "Epoch 1 Batch 400 Loss 1.8270\n",
            "Epoch 1 Loss 1.8810\n",
            "Time taken for 1 epoch 2271.0630321502686 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.7851\n",
            "Epoch 2 Batch 100 Loss 1.6902\n",
            "Epoch 2 Batch 200 Loss 1.8486\n",
            "Epoch 2 Batch 300 Loss 1.7605\n",
            "Epoch 2 Batch 400 Loss 1.6085\n",
            "Epoch 2 Loss 1.7220\n",
            "Time taken for 1 epoch 2210.1156113147736 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.6501\n",
            "Epoch 3 Batch 100 Loss 1.5824\n",
            "Epoch 3 Batch 200 Loss 1.5858\n",
            "Epoch 3 Batch 300 Loss 1.4958\n",
            "Epoch 3 Batch 400 Loss 1.6879\n",
            "Epoch 3 Loss 1.6240\n",
            "Time taken for 1 epoch 2227.5665616989136 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5278\n",
            "Epoch 4 Batch 100 Loss 1.4338\n",
            "Epoch 4 Batch 200 Loss 1.5110\n",
            "Epoch 4 Batch 300 Loss 1.4896\n",
            "Epoch 4 Batch 400 Loss 1.5147\n",
            "Epoch 4 Loss 1.5385\n",
            "Time taken for 1 epoch 2228.113371372223 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4927\n",
            "Epoch 5 Batch 100 Loss 1.5621\n",
            "Epoch 5 Batch 200 Loss 1.4523\n",
            "Epoch 5 Batch 300 Loss 1.3991\n",
            "Epoch 5 Batch 400 Loss 1.4881\n",
            "Epoch 5 Loss 1.4568\n",
            "Time taken for 1 epoch 2295.577440738678 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3736\n",
            "Epoch 6 Batch 100 Loss 1.3813\n",
            "Epoch 6 Batch 200 Loss 1.2975\n",
            "Epoch 6 Batch 300 Loss 1.3338\n",
            "Epoch 6 Batch 400 Loss 1.3246\n",
            "Epoch 6 Loss 1.3747\n",
            "Time taken for 1 epoch 2250.2399253845215 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3441\n",
            "Epoch 7 Batch 100 Loss 1.1501\n",
            "Epoch 7 Batch 200 Loss 1.3469\n",
            "Epoch 7 Batch 300 Loss 1.2320\n",
            "Epoch 7 Batch 400 Loss 1.2907\n",
            "Epoch 7 Loss 1.2947\n",
            "Time taken for 1 epoch 2237.279317140579 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.0582\n",
            "Epoch 8 Batch 100 Loss 1.2113\n",
            "Epoch 8 Batch 200 Loss 1.1554\n",
            "Epoch 8 Batch 300 Loss 1.2694\n",
            "Epoch 8 Batch 400 Loss 1.1161\n",
            "Epoch 8 Loss 1.2165\n",
            "Time taken for 1 epoch 2247.0151364803314 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.0945\n",
            "Epoch 9 Batch 100 Loss 1.2871\n",
            "Epoch 9 Batch 200 Loss 1.1193\n",
            "Epoch 9 Batch 300 Loss 0.9483\n",
            "Epoch 9 Batch 400 Loss 1.1406\n",
            "Epoch 9 Loss 1.1404\n",
            "Time taken for 1 epoch 2255.573710203171 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.0224\n",
            "Epoch 10 Batch 100 Loss 1.0561\n",
            "Epoch 10 Batch 200 Loss 1.1476\n",
            "Epoch 10 Batch 300 Loss 1.1836\n",
            "Epoch 10 Batch 400 Loss 1.0465\n",
            "Epoch 10 Loss 1.0684\n",
            "Time taken for 1 epoch 2256.119349002838 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "O3fzEjp4Dntf"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aq_E0AWZDntg"
      },
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AsG3xcdDntg",
        "outputId": "8b1a1f4d-1828-4679-efa3-ce84270db45b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fea214d2fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ASyOQMnDntg",
        "outputId": "9faa01e1-5b0c-478d-b3d4-a317f82b5123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: politicians do not have permission to do what needs to be done .\n",
            "Predicted translation: वो नहीं है कि यह नहीं है कि वो नहीं है । <end> \n"
          ]
        }
      ],
      "source": [
        "translate(u'politicians do not have permission to do what needs to be done.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}